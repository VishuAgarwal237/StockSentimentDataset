{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722551fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f37206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "from transformers.integrations import MLflowCallback\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9252c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "os.environ['HF_MLFLOW_LOG_ARTIFACTS'] = \"0\"\n",
    "print(os.getenv(\"HF_MLFLOW_LOG_ARTIFACTS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f73cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c6ce38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://ai-engg-ml-models/3', experiment_id='3', lifecycle_stage='active', name='transformer', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"https://mlflow-c3dev.concertohealth.ai\"))\n",
    "mlflow.set_experiment(experiment_name=\"transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9942ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model_name = \"bert-base-cased\"\n",
    "model_check_point = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0b3a7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_check_point)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "689523fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.label2id = {\n",
    "    'G': 1,\n",
    "    \"B\": 2,\n",
    "    \"N\" : 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f770310",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label = {\n",
    "    1: \"G\",\n",
    "    2: \"B\",\n",
    "    0: \"N\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87988ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./train_small_v4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4070254",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd8108d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22c11a43c784d148ce14cb28a7a2109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16af6c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a4e799e368493f9953a350752231fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_map = tokenized_datasets.map(lambda x: {'label':model.config.label2id[x['label']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aea38a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\"test_trainer\", \n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  num_train_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b72013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, \n",
    "                  args=training_args,\n",
    "                  train_dataset=encoded_map,\n",
    "                  eval_dataset=encoded_map,\n",
    "                  compute_metrics=compute_metrics,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d6bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence.\n",
      "/home/hnamburi@ConcertoHealth.AI/anaconda3/envs/fai/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 498\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='510' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [510/630 1:26:37 < 20:27, 0.10 it/s, Epoch 8.08/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.604542</td>\n",
       "      <td>0.743324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.290966</td>\n",
       "      <td>0.912871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.226596</td>\n",
       "      <td>0.949840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.140326</td>\n",
       "      <td>0.959898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.089782</td>\n",
       "      <td>0.973784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.069818</td>\n",
       "      <td>0.979902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.053287</td>\n",
       "      <td>0.985926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.300300</td>\n",
       "      <td>0.049494</td>\n",
       "      <td>0.987968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-500\n",
      "Configuration saved in test_trainer/checkpoint-500/config.json\n",
      "Model weights saved in test_trainer/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 498\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eedf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"buy_sell_v8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb91408c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
